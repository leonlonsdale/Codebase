---
type: note
tags:
  - type/note
  - dom/code
  - cat/logic
---
# Big O Notation

## Description

Big O notation is a mathematical concept used to analyse how an algorithm's performance changes as its input size grows. It describes the relationship between input size and a functionâ€™s execution time or memory usage.

Big O focuses on general growth patterns (constant, linear, logarithmic, quadratic, etc.) rather than exact measurements. It describes the algorithm itself, independent of hardware or implementation details.

Big O notation is written as **O(f(n))**, where: 

- **O** describes the upper bound of growth.
- **f(n)** represents how runtime or space changes relative to input size **n**.

## Examples



## Resources

